{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-step learning\n",
    "N-step learning is like traditional TD (Temporal Difference)-learning, except we do the $Reward(s,a)+\\gamma Q(s',a')$ step more than once. In fact, N-step learning would be basically the same as TD-learning if $N=1$ ($N$ is the n. of steps we look-forward).\n",
    "\n",
    "Since we'll be training on these new types of memory samples, our neural network will learn to predict the Q-value looking N steps ahead (except for those samples that end in less than N steps ahead; this is something we want anyway, because that should correlate with potentially less reward \\[if it's strictly positive in this ex.\\]).\n",
    "- good resource: https://gibberblot.github.io/rl-notes/single-agent/n-step.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization, imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x1fb1d18a250>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_ID = '07'\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers.record_video import RecordVideo\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from typing import Dict, List, Tuple, Deque\n",
    "from collections import deque\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    # obs_size = state/observable space size\n",
    "    def __init__(\n",
    "        self, \n",
    "        max_size: int, \n",
    "        obs_size: int, \n",
    "        batch_size:int=32,\n",
    "        n_step: int = 3,\n",
    "        gamma: float = 0.99,\n",
    "    ):\n",
    "        self.state_buf = np.zeros((max_size, obs_size), dtype=np.float32)\n",
    "        self.action_buf = np.zeros(max_size, dtype=np.float32)\n",
    "        self.reward_buf = np.zeros(max_size, dtype=np.float32)\n",
    "        self.ns_buf = np.zeros((max_size, obs_size), dtype=np.float32)\n",
    "        self.done_buf = np.zeros(max_size, dtype=np.float32)\n",
    "\n",
    "        self.max_size, self.batch_size = max_size, batch_size\n",
    "        self.ptr, self.size = 0, 0\n",
    "\n",
    "        self.n_step_buffer = deque(maxlen=n_step)\n",
    "        self.n_step = n_step\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def push(self, \n",
    "        state: np.ndarray, \n",
    "        action: int, \n",
    "        reward: float, \n",
    "        ns: np.ndarray, \n",
    "        done: bool,\n",
    "    ):\n",
    "        transition = (state, action, reward, ns, done)\n",
    "        self.n_step_buffer.append(transition)\n",
    "\n",
    "        if len(self.n_step_buffer) < self.n_step:\n",
    "            # we don't have to flush the n_step buffer or anything like that\n",
    "            # because it's a deque. see: https://www.geeksforgeeks.org/deque-in-python/\n",
    "            return ()\n",
    "\n",
    "        # at this point, we're now storing n-step transitions into memory\n",
    "        reward, ns, done = self._get_n_step_info(\n",
    "            self.n_step_buffer, self.gamma\n",
    "        )\n",
    "\n",
    "        # pair rew, n_s, done with a state and action that are at the start the chain\n",
    "        state, action = self.n_step_buffer[0][:2]\n",
    "\n",
    "        idx = self.ptr\n",
    "\n",
    "        self.state_buf[idx] = state\n",
    "        self.action_buf[idx] = action\n",
    "        self.reward_buf[idx] = reward\n",
    "        self.ns_buf[idx] = ns\n",
    "        self.done_buf[idx] = done\n",
    "        \n",
    "        self.ptr = (idx + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "        return self.n_step_buffer[0]\n",
    "    \n",
    "    def sample(self) -> Dict[str, np.ndarray]:\n",
    "        idxs = random.sample(range(self.size), self.batch_size)\n",
    "        return dict(\n",
    "            state=self.state_buf[idxs],\n",
    "            action=self.action_buf[idxs],\n",
    "            reward=self.reward_buf[idxs],\n",
    "            ns=self.ns_buf[idxs],\n",
    "            done=self.done_buf[idxs],\n",
    "            idxs=idxs,\n",
    "        )\n",
    "\n",
    "    def sample_from_idxs(self, idxs: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "        return dict(\n",
    "            state=self.state_buf[idxs],\n",
    "            action=self.action_buf[idxs],\n",
    "            reward=self.reward_buf[idxs],\n",
    "            ns=self.ns_buf[idxs],\n",
    "            done=self.done_buf[idxs],\n",
    "            idxs=idxs,\n",
    "        )\n",
    "\n",
    "    def _get_n_step_info(\n",
    "        self, n_step_buffer: Deque, gamma: float\n",
    "    ):\n",
    "        rew, next_state, done = n_step_buffer[-1][2:]\n",
    "\n",
    "        # we calculate it backwards\n",
    "        # this does not create correlations between different training sessions,\n",
    "        # because by multiplying by (1-done) we mask the following training session anyway\n",
    "        for transition in reversed(list(n_step_buffer)[:-1]):\n",
    "            r, n_s, d = transition[2:]\n",
    "\n",
    "            rew = r + self.gamma * (1-done) * rew\n",
    "            # if our episode ends before this \"chain\" does, then next_state will be\n",
    "            # the ending state, rather than that of the end of this chain of steps.\n",
    "            next_state, done = (n_s, d) if d else (next_state, done)\n",
    "\n",
    "        return rew, next_state, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_size,\n",
    "        out_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, out_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layers(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "I wonder if we could also store the n. of steps the deque of transitions for some sample went down (some may end prematurely) and then, when we're about to calculate the loss function, divide $gamma$ (parameter for `Agent.calculate_dqn`) by that instead of always dividing it by the size of the deque?\n",
    "\n",
    "That would seem more correct to me. Just a thought!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self,\n",
    "        env: gym.Env,\n",
    "        seed: int,\n",
    "        memory_size: int,\n",
    "        batch_size: int,\n",
    "        target_update: int,\n",
    "        eps_decay: float,\n",
    "        max_eps: float = .9,\n",
    "        min_eps: float = .1,\n",
    "        gamma: float = .99,\n",
    "        n_step: int = 3,\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.seed = seed\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        self.eps_decay = eps_decay\n",
    "        self.eps = max_eps\n",
    "        self.max_eps = max_eps\n",
    "        self.min_eps = min_eps\n",
    "        self.gamma = gamma\n",
    "        obs_size = env.observation_space.shape[0]\n",
    "        action_size = env.action_space.n\n",
    "\n",
    "        self.use_n_step = True if n_step > 1 else False\n",
    "        # like stated on the intro -- normal TD learning is just like n-step learning if `n_step=1``\n",
    "        if self.use_n_step:\n",
    "            self.n_step = n_step\n",
    "            self.memory_n = ReplayBuffer(\n",
    "                memory_size, obs_size, batch_size, n_step=n_step, gamma=gamma\n",
    "            )\n",
    "        self.memory = ReplayBuffer(memory_size, obs_size, batch_size)\n",
    "\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.dqn = Network(obs_size, action_size).to(self.device)\n",
    "        self.dqn_target = Network(obs_size, action_size).to(self.device)\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "        self.dqn_target.eval()\n",
    "\n",
    "        self.optimizer = optim.AdamW(self.dqn.parameters(), amsgrad=True)\n",
    "\n",
    "        self.transition = []\n",
    "\n",
    "        self.is_test = False\n",
    "\n",
    "        self.eff_episode = 0\n",
    "\n",
    "    def choose_action(self, state: np.ndarray) -> int:\n",
    "        explore = self.eps > np.random.random()\n",
    "        if explore:\n",
    "            selected_action = self.env.action_space.sample()\n",
    "        if not explore or self.is_test:\n",
    "            selected_action = self.dqn(torch.FloatTensor(state)).argmax().item()\n",
    "        if not self.is_test:\n",
    "            self.transition = [state, selected_action]\n",
    "        return selected_action\n",
    "    \n",
    "    def take_step(self, action: int) -> Tuple[np.ndarray, np.float64, bool]:\n",
    "        next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "        \n",
    "        done = terminated or truncated\n",
    "        \n",
    "        if not self.is_test:\n",
    "            self.transition += [reward, next_state, done]\n",
    "            if self.use_n_step:\n",
    "                # memory_n.push will return a one-step transition\n",
    "                # we do this so we can mix 1-step and n-step memories on model update\n",
    "                one_step_transition = self.memory_n.push(*self.transition)\n",
    "            else:\n",
    "                one_step_transition = self.transition\n",
    "            \n",
    "            if one_step_transition:\n",
    "                self.memory.push(*one_step_transition)\n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def compute_dqn(\n",
    "        self, \n",
    "        samples: dict[str, np.array],\n",
    "        gamma: float\n",
    "    ):\n",
    "        \"\"\"calculate the DQN loss for a batch of memories\"\"\"\n",
    "        device = self.device\n",
    "        # each of these is a batch/samples of their corresponding name\n",
    "        state = torch.FloatTensor(samples[\"state\"], device=device)\n",
    "        ns = torch.FloatTensor(samples[\"ns\"], device=device)\n",
    "        action = torch.LongTensor(samples[\"action\"], device=device)\n",
    "        reward = torch.FloatTensor(samples[\"reward\"], device=device)\n",
    "        done = torch.FloatTensor(samples[\"done\"], device=device)\n",
    "\n",
    "        curr_q_value = self.dqn(state).gather(1, action.unsqueeze(1))\n",
    "        with torch.no_grad():\n",
    "            next_q_value = self.dqn_target(ns).max(1)[0]\n",
    "        mask = 1 - done\n",
    "        target = (reward + next_q_value * gamma * mask).to(self.device)\n",
    "        \n",
    "        loss = F.smooth_l1_loss(curr_q_value.squeeze(), target)\n",
    "\n",
    "        # # for debugging!\n",
    "        # if self.eff_episode % 100 == 0:\n",
    "            # print('state', state[:2])\n",
    "            # print('ns', ns[:2])\n",
    "            # print('act', action[:2])\n",
    "            # print('rew', reward[:2])\n",
    "            # print('done', done[:2])\n",
    "            # print('curr_q', curr_q_value[:2])\n",
    "            # print('next_q', next_q_value[:2])\n",
    "            # print('target', target[:2])\n",
    "\n",
    "        return loss, torch.mean(curr_q_value).detach().numpy()\n",
    "        \n",
    "    def update_model(self) -> float:\n",
    "        samples = self.memory.sample()\n",
    "\n",
    "        loss, q_value = self.compute_dqn(samples, self.gamma)\n",
    "\n",
    "        if self.use_n_step:\n",
    "            indices = samples['idxs']\n",
    "            samples = self.memory_n.sample_from_idxs(indices)\n",
    "            gamma = self.gamma ** self.n_step\n",
    "            n_loss, _ = self.compute_dqn(samples, gamma)\n",
    "            loss += n_loss\n",
    "\n",
    "        # https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item(), q_value\n",
    "\n",
    "    def train(self, seed: int, num_frames: int, plotting_interval: int=200):\n",
    "        try:\n",
    "            self.is_test = False\n",
    "\n",
    "            state, _ = self.env.reset(seed=seed)\n",
    "            # effective episode - n. of episodes after initial \"memory gathering\"\n",
    "            total_episodes = 0\n",
    "            self.eff_episode = 0\n",
    "            score = 0\n",
    "            scores = []\n",
    "            losses = []\n",
    "            epsilons = []\n",
    "            predictions = []\n",
    "            \n",
    "            for frame_idx in range(1, num_frames+1):\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done = self.take_step(action)\n",
    "\n",
    "                state = next_state\n",
    "                score += reward\n",
    "\n",
    "                if done:\n",
    "                    total_episodes += 1\n",
    "                    state, _ = self.env.reset(seed=seed+total_episodes)\n",
    "                    scores.append(score)\n",
    "                    score = 0\n",
    "                if len(self.memory) >= self.batch_size:\n",
    "                    loss, curr_q_value = self.update_model()\n",
    "                    losses.append(loss)\n",
    "                    predictions.append(curr_q_value)\n",
    "                    self.eff_episode += 1\n",
    "                    # linear decay\n",
    "                    self.eps = max(self.min_eps, self.eps - (self.max_eps - self.min_eps) * self.eps_decay)\n",
    "                    epsilons.append(self.eps)\n",
    "\n",
    "                    if self.eff_episode % self.target_update == 0:\n",
    "                        self.target_hard_update()\n",
    "\n",
    "                if frame_idx % plotting_interval == 0:\n",
    "                    self._plot(frame_idx, scores, losses, epsilons, predictions)\n",
    "        except KeyboardInterrupt:\n",
    "            self.save_state()\n",
    "        else:\n",
    "            self.save_state('saved-state-done')\n",
    "        self.env.close()\n",
    "    \n",
    "    def save_state(self, name='saved-state'):\n",
    "        state_dict = self.dqn.state_dict()\n",
    "        torch.save(state_dict, f'saved-states/{NB_ID}.s{self.seed}.pt')\n",
    "        print('saved state!')\n",
    "        return state_dict\n",
    "\n",
    "    def load_state(self, state_dict):\n",
    "        self.dqn.load_state_dict(state_dict)\n",
    "        return state_dict\n",
    "        \n",
    "    def test(self, video_folder: str):\n",
    "        self.is_test = True\n",
    "        # save current environment to swap it back later on\n",
    "        naive_env = self.env\n",
    "        \n",
    "        self.env = gym.wrappers.RecordVideo(self.env, video_folder=video_folder, name_prefix=NB_ID)\n",
    "        state, _ = self.env.reset()\n",
    "        self.env.start_video_recorder()\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            action = self.choose_action(state)\n",
    "            next_state, reward, done = self.take_step(action)\n",
    "\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "        print(\"score: \", score)\n",
    "        self.env.close()\n",
    "\n",
    "        self.env = naive_env\n",
    "\n",
    "    def target_hard_update(self):\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "\n",
    "    def _plot(self, frame_idx, scores, losses, epsilons, predictions):\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        # taking a lil different path from the tutorial\n",
    "        # https://stackoverflow.com/questions/37970424/what-is-the-difference-between-drawing-plots-using-plot-axes-or-figure-in-matpl\n",
    "        # https://matplotlib.org/stable/_images/anatomy.png\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 5), num=1, clear=True)\n",
    "\n",
    "        ax1.set_title(f'frame {frame_idx} | score: {np.mean(scores[-10:])}')\n",
    "        ax1.plot(scores)\n",
    "        \n",
    "        ax2.set_title('loss')\n",
    "        ax2.plot(losses)\n",
    "\n",
    "        ax3.set_title('Q value')\n",
    "        ax3.plot(predictions)\n",
    "        \n",
    "        plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "env = gym.make(env_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 111\n",
    "\n",
    "def seed_torch(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    # https://pytorch.org/docs/stable/notes/randomness.html#cuda-convolution-benchmarking\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        # this will make our model run the same across multiple executions,\n",
    "        # at the cost of performance\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "np.random.seed(seed)\n",
    "seed_torch(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 30_000\n",
    "memory_size = 1_000\n",
    "batch_size = 32\n",
    "target_update = 100\n",
    "epsilon_decay = 1/2000\n",
    "\n",
    "agent = DQNAgent(env, seed, memory_size, batch_size, target_update, epsilon_decay)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved state!\n"
     ]
    }
   ],
   "source": [
    "agent.train(seed, num_frames)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_folder = 'videos'\n",
    "env_w_video = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "# NOTE: epsilon is disabled while testing\n",
    "agent = DQNAgent(env_w_video, seed, memory_size, batch_size, target_update, epsilon_decay)\n",
    "saved_sd = torch.load(f'./saved-states/{NB_ID}.s{seed}.pt') \n",
    "agent.load_state(saved_sd)\n",
    "agent.test(video_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
