{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prioritized Experience Replay (PER)\n",
    "In PER, for training, we *prioritize* selecting transitions with high TD (Temporal Difference) error, which can be comparable to the \"unexpectedness\" of a state following some action.\n",
    "This alone adds some bias during the training, because we'll be updating the model more often based on a more select set of transitions, introducing a form of overfitting. \n",
    "\n",
    "To compensate for this, we decrease the effect of those high selectance samples with *weights* on the moment that the loss is backpropagated.\n",
    "\n",
    "In a nutshell, we select more of a few samples, and to not \"pull\" the model too much into fitting mostly these (that is, overfitting), we introduce weights.\n",
    "\n",
    "- Good resource: https://danieltakeshi.github.io/2019/07/14/per/\n",
    "- Paper: https://arxiv.org/abs/1511.05952v4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x1b59e742a90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_ID = '03'\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers.record_video import RecordVideo\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from segment_tree import MinSegmentTree, SumSegmentTree\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, obs_rank: int, max_size: int, batch_size:int=32):\n",
    "        self.state_buf = np.zeros((max_size, obs_rank), dtype=np.float32)\n",
    "        self.action_buf = np.zeros(max_size, dtype=np.float32)\n",
    "        self.reward_buf = np.zeros(max_size, dtype=np.float32)\n",
    "        self.ns_buf = np.zeros((max_size, obs_rank), dtype=np.float32)\n",
    "        self.done_buf = np.zeros(max_size, dtype=np.float32)\n",
    "\n",
    "        self.max_size, self.batch_size = max_size, batch_size\n",
    "        self.ptr, self.size = 0, 0\n",
    "\n",
    "    def push(self, \n",
    "        state: np.ndarray, \n",
    "        action: int, \n",
    "        reward: float, \n",
    "        ns: np.ndarray, \n",
    "        done: bool,\n",
    "    ):\n",
    "        idx = self.ptr\n",
    "        self.state_buf[idx] = state\n",
    "        self.action_buf[idx] = action\n",
    "        self.reward_buf[idx] = reward\n",
    "        self.ns_buf[idx] = ns\n",
    "        self.done_buf[idx] = done\n",
    "        \n",
    "        self.ptr = (idx + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "    \n",
    "    def sample(self) -> Dict[str, np.ndarray]:\n",
    "        idx = random.sample(range(self.size), self.batch_size)\n",
    "        return dict(\n",
    "            state=self.state_buf[idx],\n",
    "            action=self.action_buf[idx],\n",
    "            reward=self.reward_buf[idx],\n",
    "            ns=self.ns_buf[idx],\n",
    "            done=self.done_buf[idx],\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritized replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (4188100792.py, line 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 50\u001b[1;36m\u001b[0m\n\u001b[1;33m    weight=np.array([self._calculate_weight(i, beta) for i in idx])\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "class PrioritizedReplayBuffer(ReplayBuffer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_rank: int,\n",
    "        max_size: int,\n",
    "        batch_size: int = 32,\n",
    "        alpha: float = 0.6\n",
    "    ):\n",
    "        assert alpha >= 0\n",
    "\n",
    "        super().__init__(obs_rank, max_size, batch_size)\n",
    "        self.max_priority, self.tree_ptr = 1.0, 0\n",
    "        self.alpha = alpha\n",
    "\n",
    "        tree_capacity = 1\n",
    "        # tree_capacity will end up larger than max_size\n",
    "        while tree_capacity < self.max_size:\n",
    "            tree_capacity *= 2\n",
    "\n",
    "        self.sum_tree = SumSegmentTree(tree_capacity)\n",
    "        self.min_tree = MinSegmentTree(tree_capacity)\n",
    "    \n",
    "    def push(\n",
    "        self,\n",
    "        obs: np.ndarray,\n",
    "        act: int,\n",
    "        rew: float,\n",
    "        next_obs: np.ndarray,\n",
    "        done: bool\n",
    "    ):\n",
    "        super().push(obs, act, rew, next_obs, done)\n",
    "\n",
    "        # we are gonna \"interlace\" the two to calculate the TD error\n",
    "        self.sum_tree[self.tree_ptr] = self.max_priority ** self.alpha\n",
    "        self.min_tree[self.tree_ptr] = self.max_priority ** self.alpha\n",
    "        self.tree_ptr = (self.tree_ptr + 1) % self.max_size\n",
    "\n",
    "    def sample_batch(self, beta: float = 0.4) -> Dict[str, np.ndarray]:\n",
    "        assert len(self) >= self.batch_size\n",
    "        assert beta > 0\n",
    "\n",
    "        idx = self._sample_proportional()\n",
    "\n",
    "        return dict(\n",
    "            state=self.state_buf[idx],\n",
    "            action=self.action_buf[idx],\n",
    "            reward=self.reward_buf[idx],\n",
    "            ns=self.ns_buf[idx],\n",
    "            done=self.done_buf[idx],\n",
    "            weight=np.array([self._calculate_weight(i, beta) for i in idx]),\n",
    "            idx=idx,\n",
    "        )\n",
    "\n",
    "    def _sample_proportional(self) -> List[int]:\n",
    "        indices = []\n",
    "        # we insert end here because the size of our \n",
    "        # memory will be smaller than the tree\n",
    "        # NOTE: I think it wold be fine if we ommitted these parameters,\n",
    "        # if we initialize the tree with values 0\n",
    "        p_total = self.sum_tree.sum(start=0, end=len(self) - 1)\n",
    "        # divide total td sum over each batch\n",
    "        segment = p_total / self.batch_size\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            upperbound = random.uniform(a, b)\n",
    "            idx = self.sum_tree.find_prefixsum_idx(upperbound)\n",
    "            indices.append(idx)\n",
    "\n",
    "        return indices\n",
    "\n",
    "    def update_priorities(self, indices: List[int], priorities:np.ndarray):\n",
    "        assert len(indices) == len(priorities)\n",
    "\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            assert priority > 0\n",
    "            assert 0 <= idx < len(self)\n",
    "\n",
    "            self.sum_tree[idx] = priority ** self.alpha\n",
    "            self.min_tree[idx] = priority ** self.alpha\n",
    "\n",
    "            self.max_priority = max(self.max_priority, priority)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_size,\n",
    "        out_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, out_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layers(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self,\n",
    "        env: gym.Env,\n",
    "        seed: int,\n",
    "        memory_size: int,\n",
    "        batch_size: int,\n",
    "        target_update: int,\n",
    "        eps_decay: float,\n",
    "        max_eps: float = .9,\n",
    "        min_eps: float = .1,\n",
    "        gamma: float = .99,\n",
    "        # PER parameters\n",
    "        # selection bias (less is more)\n",
    "        # NOTE: alpha doe snot get updated\n",
    "        alpha: float = 0.2,\n",
    "        # weight factor\n",
    "        beta: float = 0.6,\n",
    "        prior_eps: float = 1e-6,\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.seed = seed\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        self.eps_decay = eps_decay\n",
    "        self.eps = max_eps\n",
    "        self.max_eps = max_eps\n",
    "        self.min_eps = min_eps\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.beta = beta\n",
    "        self.prior_eps = prior_eps\n",
    "\n",
    "        obs_size = env.observation_space.shape[0]\n",
    "        action_size = env.action_space.n\n",
    "\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        \n",
    "        self.memory = PrioritizedReplayBuffer(memory_size, obs_size, batch_size, alpha)\n",
    "        \n",
    "        self.dqn = Network(obs_size, action_size).to(self.device)\n",
    "        self.dqn_target = Network(obs_size, action_size).to(self.device)\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "        self.dqn_target.eval()\n",
    "\n",
    "        # in the original tutorial, LR is not initialized.. but I think we're fine?\n",
    "        # self.optimizer = optim.AdamW(self.dqn.parameters(), lr=self.lr, amsgrad=True)\n",
    "        self.optimizer = optim.AdamW(self.dqn.parameters(), amsgrad=True)\n",
    "\n",
    "        self.transition = []\n",
    "\n",
    "        self.is_test = False\n",
    "\n",
    "    def choose_action(self, state: np.ndarray) -> int:\n",
    "        explore = self.eps > np.random.random()\n",
    "        if explore:\n",
    "            selected_action = self.env.action_space.sample()\n",
    "        if not explore or self.is_test:\n",
    "            selected_action = self.dqn(torch.FloatTensor(state)).argmax().item()\n",
    "        if not self.is_test:\n",
    "            self.transition = [state, selected_action]\n",
    "        return selected_action\n",
    "    \n",
    "    def take_step(self, action: int) -> Tuple[np.ndarray, np.float64, bool]:\n",
    "        next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "        \n",
    "        done = terminated or truncated\n",
    "        \n",
    "        if not self.is_test:\n",
    "            self.transition += [reward, next_state, done]\n",
    "            self.memory.push(*self.transition)\n",
    "            \n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def compute_dqn(self, samples: dict[str, np.array]):\n",
    "        \"\"\"calculate the DQN loss for a batch of memories\"\"\"\n",
    "        device = self.device\n",
    "        # each of these is a batch/samples of their corresponding name\n",
    "        state = torch.FloatTensor(samples[\"state\"], device=device)\n",
    "        ns = torch.FloatTensor(samples[\"ns\"], device=device)\n",
    "        action = torch.LongTensor(samples[\"action\"], device=device)\n",
    "        reward = torch.FloatTensor(samples[\"reward\"], device=device)\n",
    "        done = torch.FloatTensor(samples[\"done\"], device=device)\n",
    "\n",
    "        # we need to unsqueeze actions because gather requires the 2 tensors to be of same shape,\n",
    "        # even if the relevant idx's dimension has size 1.\n",
    "        # and then we squeeze so that:\n",
    "        # [[q1], [q2], [qn]] -> [q1, q2, qn]\n",
    "        # ^ the output is like this because tensorflow retains the shape\n",
    "        curr_q_value = self.dqn(state).gather(1, action.unsqueeze(1)).squeeze()\n",
    "        with torch.no_grad():\n",
    "            # get q-values tensor for action space\n",
    "            # these 2 are equivalent\n",
    "            # next_actions = self.dqn(ns).argmax(1).unsqueeze(1)\n",
    "            next_actions = self.dqn(ns).argmax(1, keepdim=True)\n",
    "            next_q_values = self.dqn_target(ns)\n",
    "            next_q_value = next_q_values.gather(1, next_actions).squeeze()\n",
    "        mask = 1 - done\n",
    "        target = (reward + next_q_value * self.gamma * mask).to(self.device)\n",
    "        elementwise_loss = F.smooth_l1_loss(curr_q_value, target, reduction='none')\n",
    "        \n",
    "        # # for debugging!\n",
    "        # print('state', state[:2])\n",
    "        # print('ns', ns[:2])\n",
    "        # print('act', action[:2])\n",
    "        # print('rew', reward[:2])\n",
    "        # print('done', done[:2])\n",
    "        # print('curr_q', curr_q_value[:2])\n",
    "        # print('curr_q_sqz', curr_q_value.squeeze(0)[:2])\n",
    "        # print('next_act', next_actions[:2])\n",
    "        # print('next_qs', next_q_values[:2])\n",
    "        # print('next_q', next_q_value[:2])\n",
    "        # print('target', target[:2])\n",
    "\n",
    "        mean_q_value = torch.mean(curr_q_value).detach().numpy()\n",
    "        return elementwise_loss, mean_q_value\n",
    "        \n",
    "    def update_model(self) -> float:\n",
    "        samples = self.memory.sample(self.beta)\n",
    "        weights = torch.FloatTensor(samples['weights'], device=device).unsqueeze(0)\n",
    "        indices = samples['indices']\n",
    "\n",
    "        elementwise_loss, q_value = self.compute_dqn(samples)\n",
    "        loss = torch.mean(elementwise_loss * weights)\n",
    "\n",
    "        # if in doubt, check comments on cartpole dqn\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # NOTE: we use the new priorities from BEFORE the model gets updated (backprop).\n",
    "        # it seems to me that it would make more sense to feed back the weights\n",
    "        # from AFTER the update, but then we'd have to calculate the Q-values twice.\n",
    "\n",
    "        # turn into numpy array\n",
    "        loss_for_prior = elementwise_loss.detach().cpu().numpy()\n",
    "        new_priorities = loss_for_prior + self.prior_eps\n",
    "        self.memory.update_priorities(indices, new_priorities)\n",
    "\n",
    "        return loss.item(), q_value\n",
    "\n",
    "    def train(self, seed: int, num_frames: int, plotting_interval: int=200):\n",
    "        try:\n",
    "            self.is_test = False\n",
    "\n",
    "            state, _ = self.env.reset(seed=seed)\n",
    "            # effective episode - n. of episodes after initial \"memory gathering\"\n",
    "            total_episodes = 0\n",
    "            eff_episode = 0\n",
    "            score = 0\n",
    "            scores = []\n",
    "            losses = []\n",
    "            epsilons = []\n",
    "            predictions = []\n",
    "            \n",
    "            for frame_idx in range(1, num_frames+1):\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done = self.take_step(action)\n",
    "\n",
    "                state = next_state\n",
    "                score += reward\n",
    "\n",
    "                fraction = min(frame_idx / num_frames, 1)\n",
    "                # NOTE: I don't know if this is supposed to be linear,\n",
    "                # because this isn't the case\n",
    "                self.beta = self.beta + (self.beta - 1) * fraction\n",
    "\n",
    "                if done:\n",
    "                    total_episodes += 1\n",
    "                    state, _ = self.env.reset(seed=seed+total_episodes)\n",
    "                    scores.append(score)\n",
    "                    score = 0\n",
    "                if len(self.memory) >= self.batch_size:\n",
    "                    loss, curr_q_value = self.update_model()\n",
    "                    losses.append(loss)\n",
    "                    predictions.append(curr_q_value)\n",
    "                    eff_episode += 1\n",
    "                    # linear decay\n",
    "                    self.eps = max(self.min_eps, self.eps - (self.max_eps - self.min_eps) * self.eps_decay)\n",
    "                    epsilons.append(self.eps)\n",
    "\n",
    "                    if eff_episode % self.target_update == 0:\n",
    "                        self.target_hard_update()\n",
    "\n",
    "                if frame_idx % plotting_interval == 0:\n",
    "                    self._plot(frame_idx, scores, losses, epsilons, predictions)\n",
    "        except KeyboardInterrupt:\n",
    "            self.save_state()\n",
    "        else:\n",
    "            self.save_state('saved-state-done')\n",
    "        self.env.close()\n",
    "    \n",
    "    def save_state(self, name='saved-state'):\n",
    "        state_dict = self.dqn.state_dict()\n",
    "        torch.save(state_dict, f'saved-states/{NB_ID}.s{self.seed}.pt')\n",
    "        print('saved state!')\n",
    "        return state_dict\n",
    "\n",
    "    def load_state(self, state_dict):\n",
    "        self.dqn.load_state_dict(state_dict)\n",
    "        return state_dict\n",
    "        \n",
    "    def test(self, video_folder: str):\n",
    "        self.is_test = True\n",
    "        # save current environment to swap it back later on\n",
    "        naive_env = self.env\n",
    "        \n",
    "        # self.env = RecordVideo(self.env, video_folder=video_folder, episode_trigger=lambda x:True)\n",
    "        self.env = gym.wrappers.RecordVideo(self.env, video_folder=video_folder, name_prefix=NB_ID)\n",
    "        state, _ = self.env.reset()\n",
    "        self.env.start_video_recorder()\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            action = self.choose_action(state)\n",
    "            next_state, reward, done = self.take_step(action)\n",
    "\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "        print(\"score: \", score)\n",
    "        self.env.close()\n",
    "\n",
    "        self.env = naive_env\n",
    "\n",
    "    def target_hard_update(self):\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "\n",
    "    def _plot(self, frame_idx, scores, losses, epsilons, predictions):\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        # taking a lil different path from the tutorial\n",
    "        # https://stackoverflow.com/questions/37970424/what-is-the-difference-between-drawing-plots-using-plot-axes-or-figure-in-matpl\n",
    "        # https://matplotlib.org/stable/_images/anatomy.png\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 5), num=1, clear=True)\n",
    "        ax1.set_title(f'frame {frame_idx} | avg. t-10 score: {np.mean(scores[-10:])}')\n",
    "        ax1.plot(scores)\n",
    "        ax1.set_xlabel('episode')\n",
    "        ax1.set_ylabel('score')\n",
    "        ax2.set_title('loss')\n",
    "        ax2.plot(losses)\n",
    "        # ax3.set_title('epsilons')\n",
    "        ax3.set_title('Q value')\n",
    "        ax3.plot(predictions)\n",
    "        plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "env = gym.make(env_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 111\n",
    "\n",
    "def seed_torch(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    # https://pytorch.org/docs/stable/notes/randomness.html#cuda-convolution-benchmarking\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        # this will make our model run the same across multiple executions,\n",
    "        # at the cost of performance\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "np.random.seed(seed)\n",
    "seed_torch(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 30_000\n",
    "memory_size = 1_000\n",
    "batch_size = 32\n",
    "target_update = 100\n",
    "epsilon_decay = 1/2000\n",
    "\n",
    "agent = DQNAgent(env, seed, memory_size, batch_size, target_update, epsilon_decay)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(seed, num_frames)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing + recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_folder = 'videos'\n",
    "env_w_video = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "# NOTE: epsilon is disabled while testing\n",
    "agent = DQNAgent(env_w_video, seed, memory_size, batch_size, target_update, epsilon_decay)\n",
    "saved_sd = torch.load(f'saved-states/{NB_ID}.s{seed}.pt') \n",
    "agent.load_state(saved_sd)\n",
    "agent.test(video_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
