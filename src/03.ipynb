{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prioritized Experience Replay (PER)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_ID = '03'\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers.record_video import RecordVideo\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size: int, obs_size: int, batch_size:int=32):\n",
    "        self.state_buf = np.zeros((max_size, obs_size), dtype=np.float32)\n",
    "        self.action_buf = np.zeros(max_size, dtype=np.float32)\n",
    "        self.reward_buf = np.zeros(max_size, dtype=np.float32)\n",
    "        self.ns_buf = np.zeros((max_size, obs_size), dtype=np.float32)\n",
    "        self.done_buf = np.zeros(max_size, dtype=np.float32)\n",
    "\n",
    "        self.max_size, self.batch_size = max_size, batch_size\n",
    "        self.ptr, self.size = 0, 0\n",
    "\n",
    "    def push(self, \n",
    "        state: np.ndarray, \n",
    "        action: int, \n",
    "        reward: float, \n",
    "        ns: np.ndarray, \n",
    "        done: bool,\n",
    "    ):\n",
    "        idx = self.ptr\n",
    "        self.state_buf[idx] = state\n",
    "        self.action_buf[idx] = action\n",
    "        self.reward_buf[idx] = reward\n",
    "        self.ns_buf[idx] = ns\n",
    "        self.done_buf[idx] = done\n",
    "        \n",
    "        self.ptr = (idx + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "    \n",
    "    def sample(self) -> Dict[str, np.ndarray]:\n",
    "        idx = random.sample(range(self.size), self.batch_size)\n",
    "        return dict(\n",
    "            state=self.state_buf[idx],\n",
    "            action=self.action_buf[idx],\n",
    "            reward=self.reward_buf[idx],\n",
    "            ns=self.ns_buf[idx],\n",
    "            done=self.done_buf[idx],\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_size,\n",
    "        out_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, out_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layers(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self,\n",
    "        env: gym.Env,\n",
    "        seed: int,\n",
    "        memory_size: int,\n",
    "        batch_size: int,\n",
    "        target_update: int,\n",
    "        eps_decay: float,\n",
    "        max_eps: float = .9,\n",
    "        min_eps: float = .1,\n",
    "        gamma: float = .99,\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.seed = seed\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        self.eps_decay = eps_decay\n",
    "        self.eps = max_eps\n",
    "        self.max_eps = max_eps\n",
    "        self.min_eps = min_eps\n",
    "        self.gamma = gamma\n",
    "        obs_size = env.observation_space.shape[0]\n",
    "        action_size = env.action_space.n\n",
    "\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        \n",
    "        self.memory = ReplayBuffer(10000, obs_size, batch_size)\n",
    "        \n",
    "        self.dqn = Network(obs_size, action_size).to(self.device)\n",
    "        self.dqn_target = Network(obs_size, action_size).to(self.device)\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "        self.dqn_target.eval()\n",
    "\n",
    "        # in the original tutorial, LR is not initialized.. but I think we're fine?\n",
    "        # self.optimizer = optim.AdamW(self.dqn.parameters(), lr=self.lr, amsgrad=True)\n",
    "        self.optimizer = optim.AdamW(self.dqn.parameters(), amsgrad=True)\n",
    "\n",
    "        self.transition = []\n",
    "\n",
    "        self.is_test = False\n",
    "\n",
    "    def choose_action(self, state: np.ndarray) -> int:\n",
    "        explore = self.eps > np.random.random()\n",
    "        if explore:\n",
    "            selected_action = self.env.action_space.sample()\n",
    "        if not explore or self.is_test:\n",
    "            selected_action = self.dqn(torch.FloatTensor(state)).argmax().item()\n",
    "        if not self.is_test:\n",
    "            self.transition = [state, selected_action]\n",
    "        return selected_action\n",
    "    \n",
    "    def take_step(self, action: int) -> Tuple[np.ndarray, np.float64, bool]:\n",
    "        next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "        \n",
    "        done = terminated or truncated\n",
    "        \n",
    "        if not self.is_test:\n",
    "            self.transition += [reward, next_state, done]\n",
    "            self.memory.push(*self.transition)\n",
    "            \n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def compute_dqn(self, samples: dict[str, np.array]):\n",
    "        \"\"\"calculate the DQN loss for a batch of memories\"\"\"\n",
    "        device = self.device\n",
    "        # each of these is a batch/samples of their corresponding name\n",
    "        state = torch.FloatTensor(samples[\"state\"], device=device)\n",
    "        ns = torch.FloatTensor(samples[\"ns\"], device=device)\n",
    "        action = torch.LongTensor(samples[\"action\"], device=device)\n",
    "        reward = torch.FloatTensor(samples[\"reward\"], device=device)\n",
    "        done = torch.FloatTensor(samples[\"done\"], device=device)\n",
    "\n",
    "        # we squeeze so that:\n",
    "        # [[q1], [q2], [qn]] -> [q1, q2, qn]\n",
    "        # we need to do this because gather requires the 2 tensors to be of same shape,\n",
    "        # even if the relevant idx's dimension has size 1\n",
    "        curr_q_value = self.dqn(state).gather(1, action.unsqueeze(1)).squeeze()\n",
    "        with torch.no_grad():\n",
    "            # get q-values tensor for action space\n",
    "            # these 2 are equivalent\n",
    "            # next_actions = self.dqn(ns).argmax(1).unsqueeze(1)\n",
    "            next_actions = self.dqn(ns).argmax(1, keepdim=True)\n",
    "            next_q_values = self.dqn_target(ns)\n",
    "            next_q_value = next_q_values.gather(1, next_actions).squeeze()\n",
    "        mask = 1 - done\n",
    "        target = (reward + next_q_value * self.gamma * mask).to(self.device)\n",
    "        loss = F.smooth_l1_loss(curr_q_value, target)\n",
    "        \n",
    "        # # for debugging!\n",
    "        # print('state', state[:2])\n",
    "        # print('ns', ns[:2])\n",
    "        # print('act', action[:2])\n",
    "        # print('rew', reward[:2])\n",
    "        # print('done', done[:2])\n",
    "        # print('curr_q', curr_q_value[:2])\n",
    "        # print('curr_q_sqz', curr_q_value.squeeze(0)[:2])\n",
    "        # print('next_act', next_actions[:2])\n",
    "        # print('next_qs', next_q_values[:2])\n",
    "        # print('next_q', next_q_value[:2])\n",
    "        # print('target', target[:2])\n",
    "\n",
    "        mean_q_value = torch.mean(curr_q_value).detach().numpy()\n",
    "        return loss, mean_q_value\n",
    "        \n",
    "    def update_model(self) -> float:\n",
    "        samples = self.memory.sample()\n",
    "\n",
    "        loss, q_value = self.compute_dqn(samples)\n",
    "\n",
    "        # if in doubt, check comments on cartpole dqn\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item(), q_value\n",
    "\n",
    "    def train(self, seed: int, num_frames: int, plotting_interval: int=200):\n",
    "        try:\n",
    "            self.is_test = False\n",
    "\n",
    "            state, _ = self.env.reset(seed=seed)\n",
    "            # effective episode - n. of episodes after initial \"memory gathering\"\n",
    "            total_episodes = 0\n",
    "            eff_episode = 0\n",
    "            score = 0\n",
    "            scores = []\n",
    "            losses = []\n",
    "            epsilons = []\n",
    "            predictions = []\n",
    "            \n",
    "            for frame_idx in range(1, num_frames+1):\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done = self.take_step(action)\n",
    "\n",
    "                state = next_state\n",
    "                score += reward\n",
    "\n",
    "                if done:\n",
    "                    total_episodes += 1\n",
    "                    # TODO: make gym reproducible here. problem is if we use `seed` we're always getting the same environment\n",
    "                    # a solution could be self.env.reset(seed=seed+total_episodes)\n",
    "                    state, _ = self.env.reset(seed=seed+total_episodes)\n",
    "                    scores.append(score)\n",
    "                    score = 0\n",
    "                # we will first let it collect a few memories,\n",
    "                # and THEN start optimizing.\n",
    "                if len(self.memory) >= self.batch_size:\n",
    "                    loss, curr_q_value = self.update_model()\n",
    "                    losses.append(loss)\n",
    "                    predictions.append(curr_q_value)\n",
    "                    eff_episode += 1\n",
    "                    # linear decay\n",
    "                    self.eps = max(self.min_eps, self.eps - (self.max_eps - self.min_eps) * self.eps_decay)\n",
    "                    epsilons.append(self.eps)\n",
    "\n",
    "                    if eff_episode % self.target_update == 0:\n",
    "                        self.target_hard_update()\n",
    "                    # I think I could use this but I want to be consistent with the material so nvm\n",
    "                    # self.eps = max(self.min_eps, self.eps - self.eps_decay)\n",
    "\n",
    "                if frame_idx % plotting_interval == 0:\n",
    "                    self._plot(frame_idx, scores, losses, epsilons, predictions)\n",
    "        except KeyboardInterrupt:\n",
    "            self.save_state()\n",
    "        else:\n",
    "            self.save_state('saved-state-done')\n",
    "        self.env.close()\n",
    "    \n",
    "    def save_state(self, name='saved-state'):\n",
    "        state_dict = self.dqn.state_dict()\n",
    "        torch.save(state_dict, f'saved-states/{NB_ID}.s{self.seed}.pt')\n",
    "        print('saved state!')\n",
    "        return state_dict\n",
    "\n",
    "    def load_state(self, state_dict):\n",
    "        self.dqn.load_state_dict(state_dict)\n",
    "        return state_dict\n",
    "        \n",
    "    def test(self, video_folder: str):\n",
    "        self.is_test = True\n",
    "        # save current environment to swap it back later on\n",
    "        naive_env = self.env\n",
    "        \n",
    "        # self.env = RecordVideo(self.env, video_folder=video_folder, episode_trigger=lambda x:True)\n",
    "        self.env = gym.wrappers.RecordVideo(self.env, video_folder=video_folder, name_prefix=NB_ID)\n",
    "        state, _ = self.env.reset()\n",
    "        self.env.start_video_recorder()\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            action = self.choose_action(state)\n",
    "            next_state, reward, done = self.take_step(action)\n",
    "\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "        print(\"score: \", score)\n",
    "        self.env.close()\n",
    "\n",
    "        self.env = naive_env\n",
    "\n",
    "    def target_hard_update(self):\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "\n",
    "    def _plot(self, frame_idx, scores, losses, epsilons, predictions):\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        # taking a lil different path from the tutorial\n",
    "        # https://stackoverflow.com/questions/37970424/what-is-the-difference-between-drawing-plots-using-plot-axes-or-figure-in-matpl\n",
    "        # https://matplotlib.org/stable/_images/anatomy.png\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 5), num=1, clear=True)\n",
    "        ax1.set_title(f'frame {frame_idx} | avg. t-10 score: {np.mean(scores[-10:])}')\n",
    "        ax1.plot(scores)\n",
    "        ax1.set_xlabel('episode')\n",
    "        ax1.set_ylabel('score')\n",
    "        ax2.set_title('loss')\n",
    "        ax2.plot(losses)\n",
    "        # ax3.set_title('epsilons')\n",
    "        ax3.set_title('Q value')\n",
    "        ax3.plot(predictions)\n",
    "        plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "env = gym.make(env_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 111\n",
    "\n",
    "def seed_torch(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    # https://pytorch.org/docs/stable/notes/randomness.html#cuda-convolution-benchmarking\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        # this will make our model run the same across multiple executions,\n",
    "        # at the cost of performance\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "np.random.seed(seed)\n",
    "seed_torch(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 30_000\n",
    "memory_size = 1_000\n",
    "batch_size = 32\n",
    "target_update = 100\n",
    "epsilon_decay = 1/2000\n",
    "\n",
    "agent = DQNAgent(env, seed, memory_size, batch_size, target_update, epsilon_decay)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(seed, num_frames)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing + recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_folder = 'videos'\n",
    "env_w_video = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "# NOTE: epsilon is disabled while testing\n",
    "agent = DQNAgent(env_w_video, seed, memory_size, batch_size, target_update, epsilon_decay)\n",
    "saved_sd = torch.load(f'saved-states/{NB_ID}.s{seed}.pt') \n",
    "agent.load_state(saved_sd)\n",
    "agent.test(video_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
